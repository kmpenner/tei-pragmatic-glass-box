# Encoding Accountability: Teaching Editorial Discernment Through Risk-Based AI Provenance and Stand-Off Annotation

**Format:** Long Paper
**Keywords:** TEI pedagogy; provenance; stand-off annotation; editorial workflow; uncertainty

Large language models (LLMs) can now generate syntactically valid TEI XML, propose regularizations, and suggest structural and linguistic analyses for historical texts. This capability intensifies a pedagogical and methodological anxiety within TEI practice: if a machine can produce plausible markup, what exactly are we teaching when we teach encoding? This paper argues that the necessary intervention is not to treat LLMs as transcription assistants, but to formally model them as fallible agents whose outputs constitute editorial claims requiring explicit, machine-actionable provenance and human adjudication. Unlike current practices that often relegate AI usage to informal workflows or prose-only acknowledgments, we propose a formalized, project-level, machine-readable taxonomy of accountable disagreement.

We present a TEI-native classroom workflow that treats LLM output as an auditable suggestion layer rather than as base text. Suggestions and adjudications are stored strictly as stand-off records (e.g., within standOff) linked to stable anchors in the base transcription. Students generate machine proposals for discrete editorial claims and audit those claims against witness evidence and edition policy. Crucially, we log these events at a fine granularity because that is precisely where philological assumptions hide. To prevent the unmanageable XML logging burden typical of AI tracking and to ensure classroom feasibility, each log entry relies on a "pragmatic glass box" data model featuring minimal default payloads:

*   **Methodological Citing:** Prompts are treated as citable, version-controlled editorial methods. They are declared once as "Prompt Profiles" within the TEI header (teiHeader) and referenced by `@source` at the micro-event level.
*   **Tiered Adjudication:** Every machine suggestion receives a minimal adjudication record. Most suggestions are routine and can be adjudicated with a simple decision (accept/reject/modify) and a controlled-vocabulary reason code (bounded and governed by the instructor).
*   **Risk-Based Escalation:** Free-text rationales and explicit evidence pointers are required only for contested or high-impact cases, such as rejections, unresolved ambiguities, or instructional exemplars.

A focused discussion of fine-grained linguistic claims demonstrates why this tiered, micro-level approach is necessary. LLMs frequently apply false certainty when inferring structure from surface forms—whether in universally familiar cases (e.g., contractions, clitic attachment, compound boundaries) or in higher-stakes analyses such as lemmatization and morphological tagging in highly inflected historical languages. Because downstream interpretation depends on these inferences, the classroom task is to detect overconfident suggestions, adjudicate them using controlled reason codes (e.g., #unitMismatch, #overRegularization, #lemmaError, #formAmbiguity), and encode uncertainty and responsibility explicitly rather than silently "fixing" the text.

Methodologically, this approach yields a dataset that is compact, queryable, and capable of supporting longitudinal comparison across prompt profiles and model versions. Pedagogically, it redefines TEI training for an AI-saturated environment. By shifting the center of gravity from automated efficiency to accountable disagreement, we train students to encode responsibility rather than merely produce valid XML—directly answering the TEI 2026 call to unsettle inherited practices.
